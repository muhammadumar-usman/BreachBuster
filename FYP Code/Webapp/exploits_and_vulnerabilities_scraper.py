import requests
from bs4 import BeautifulSoup
import pyxploitdb
import nvdlib
from urllib.error import HTTPError
from bs4 import BeautifulSoup


def search_cves_for_technology(keyword):
    global cve_list, cves_with_exploits, cve_details
    cve_list = []
    cves_with_exploits = []
    cve_details = {}
    temp = []
    
    
    try:
        # Search for CVEs containing the specified keyword
        r = nvdlib.searchCVE(keywordSearch=keyword, limit=1, key='0de04895-bc7a-4779-9614-2917c190199a', delay=0.6)
        
        for eachCVE in r:
            # Check if exploits exist for this CVE
            try:
                p = pyxploitdb.searchCVE(eachCVE.id)
            except HTTPError as e:
                print(f"Error retrieving exploits for CVE {eachCVE.id}: {e}")
                continue
            
            if len(p) > 0:
                cve_list.append(eachCVE)
                cves_with_exploits.append(eachCVE.id)
                print(f"CVE ID: {eachCVE.id}")
                cvss_metrics = eachCVE.score
                cvss_score = cvss_metrics[1]
                cvss_severity = cvss_metrics[2]
                print(f"Score: {cvss_score}")
                print(f"Severity: {cvss_severity}")
                print(f"CVE Link: {eachCVE.url}")
                

                temp.append(eachCVE.descriptions)
                x=temp[0][0]
                z = ''
                z += str(x)
                cve_description = ''
                cve_description += z[25:-2]
                print(f"Description: {cve_description}")
                temp = []

                cve_details.update({"CVE ID" : eachCVE.id ,"Description" : cve_description , "Publish Date" : eachCVE.published , "NVD Link" : eachCVE.url , "Score" : eachCVE.score , "CWE" : eachCVE.cwe , "Refrences" : eachCVE.references , "CPE" : eachCVE.cpe })                                                 
                
                print("-" * 50)
    
    except HTTPError as e:
        print(f"Error retrieving CVEs: {e}")
        

def print_exploits_for_cve(cve_list):
    global exploits_details_less, exploits_links
    exploits_links = []
    exploits_details_less = ''
    if not cve_list:
        print("No CVEs with associated exploits found.")
        return
    
    try:
        for cve in cve_list:
            
            try:
                p = pyxploitdb.searchCVE(cve)
            except HTTPError as e:
                print(f"Error retrieving exploits for CVE {cve}: {e}")
                continue
            
            for exploit in p:
                exploits_details_less = (f"CVE ID: {cve}" + " \n" + f"Exploits: {exploit}")
                print(exploit)
                nvd_link = str(exploit)
                #scrape_exploit_data(nvd_link)
            print("-" * 50)


            # Find the position of 'link=' in the string
            link_start_index = nvd_link.find("link=")

            if link_start_index != -1:
                # Extract the substring starting from the link value
                link_substr = nvd_link[link_start_index + 6:]  # +6 to skip 'link='
                
                # Find the closing quote (') to determine the end of the link value
                link_end_index = link_substr.find("'")
                
                if link_end_index != -1:
                    # Extract the link value
                    extracted_link = link_substr[:link_end_index]
                    exploits_links.append(extracted_link)
                    print("Extracted Link:", extracted_link)
                else:
                    print("Invalid format: Closing quote not found for link value.")
            else:
                print("Invalid format: 'link=' not found in the string.")
    
    except HTTPError as e:
        print(f"Error printing exploits: {e}")


def scrape_exploit_data(url):
    global exploit_full_code
    exploit_full_code = ''
    try:
        # Define comprehensive headers to mimic a browser request
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Accept-Language': 'en-US,en;q=0.9',
            'Referer': 'https://www.google.com/',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }

        # Send a GET request to the URL with custom headers
        response = requests.get(url, headers=headers)

        # Check if the request was successful (status code 200)
        if response.status_code == 200:
            # Parse the HTML content of the webpage
            soup = BeautifulSoup(response.content, 'html.parser')

            # Find the <code> element containing the exploit data
            code_element = soup.find('code')

            # Extract the text content of the <code> element
            if code_element:
                exploit_data = code_element.get_text()
                # return exploit_data
                scraped_data = exploit_data
                # Remove \r characters from the scraped data
                cleaned_data = scraped_data.replace('\r', '')

                # Print the cleaned data (formatted with proper line breaks)
                print("Exploit Data:")
                print(cleaned_data)
                exploit_full_code = cleaned_data
                return cleaned_data
            else:
                print("No exploit data found for this CVE")
                return None
        else:
            print(f"Failed to retrieve expliot.")
            return None

    except requests.exceptions.RequestException as e:
        print(f"Error: {e}")
        return None

